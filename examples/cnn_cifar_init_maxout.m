function net = cnn_cifar_init_maxout(opts)

lr = [.1 2] ;

% For initialization that makes it the same state with maxpool
maxpool_filter_unit = zeros([2, 2, 1, 4], 'single') ;
maxpool_filter_unit(:,:,1,1) = [1, 0; 0, 0;] ;
maxpool_filter_unit(:,:,1,2) = [0, 1; 0, 0;] ;
maxpool_filter_unit(:,:,1,3) = [0, 0; 1, 0;] ;
maxpool_filter_unit(:,:,1,4) = [0, 0; 0, 1;] ;

% Define network CIFAR10-quick-based max-pool to conv+maxout
net.layers = {} ;

% Block 1
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{0.01*randn(5,5,3,32, 'single'), zeros(1, 32, 'single')}}, ...
                           'learningRate', lr, ...
                           'stride', 1, ...
                           'pad', 2) ;
net.layers{end+1} = struct('type', 'relu') ;
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{repmat(maxpool_filter_unit, [1, 1, 1, 32]), zeros(1, 128, 'single')}}, ...
                           'learningRate', lr, ...
                           'stride', 2, ...
                           'pad', 0) ;
net.layers{end+1} = struct('type', 'maxout', 'maxout_groupsize', 4, 'maxout_mask', []) ;

% Block 2
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{0.05*randn(5,5,32,32, 'single'), zeros(1,32,'single')}}, ...
                           'learningRate', lr, ...
                           'stride', 1, ...
                           'pad', 2) ;
net.layers{end+1} = struct('type', 'relu') ;
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{repmat(maxpool_filter_unit, [1,1,1,32]), zeros(1, 128, 'single')}}, ...
                           'learningRate', lr, ...
                           'stride', 2, ...
                           'pad', 0) ;
net.layers{end+1} = struct('type', 'maxout', 'maxout_groupsize', 4, 'maxout_mask', []) ;

% Block 3
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{0.05*randn(5,5,32,64, 'single'), zeros(1,64,'single')}}, ...
                           'learningRate', lr, ...
                           'stride', 1, ...
                           'pad', 2) ;
net.layers{end+1} = struct('type', 'relu') ;
net.layers{end+1} = struct('type', 'pool', ...
                           'method', 'avg', ...
                           'pool', [3 3], ...
                           'stride', 2, ...
                           'pad', [0 1 0 1]) ; % Emulate caffe

% Block 4
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{0.05*randn(4,4,64,64, 'single'), zeros(1,64,'single')}}, ...
                           'learningRate', lr, ...
                           'stride', 1, ...
                           'pad', 0) ;
net.layers{end+1} = struct('type', 'relu') ;

% Block 5
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{0.05*randn(1,1,64,10, 'single'), zeros(1,10,'single')}}, ...
                           'learningRate', .1*lr, ...
                           'stride', 1, ...
                           'pad', 0) ;

% Loss layer
net.layers{end+1} = struct('type', 'softmaxloss') ;

